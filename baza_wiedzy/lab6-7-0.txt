-------------------------------lab6--------------------------------
import csv
import os, math, re, random, pandas as pd, time, faiss, numpy as np
from datetime import datetime
import matplotlib.pyplot as plt # pip install matplotlib

from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer

MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

TOPICS = {
    "ai": [
        "Large language models predict the next token using transformer architectures.",
        "Embeddings map text into dense vectors enabling semantic search.",
        "RAG combines retrieval with generation to ground responses."
    ],
    "sport": [
        "Marathon training plans balance long runs and recovery days.",
        "Strength training improves running economy and power.",
        "Interval sessions develop speed and lactate threshold."
    ],
    "cooking": [
        "Sourdough starter needs regular feeding to stay active.",
        "Sous vide cooking keeps precise temperatures for tenderness.",
        "Spices bloom in hot oil enhancing aroma and flavor."
    ],
    "geo": [
        "Rivers shape valleys through erosion and sediment transport.",
        "Plate tectonics explains earthquakes and mountain building.",
        "Deserts form where evaporation exceeds precipitation."
    ],
    "health": [
        "Sleep supports memory consolidation and hormonal balance.",
        "Aerobic exercise benefits cardiovascular health and VO2 max.",
        "Protein intake supports muscle repair and satiety."
    ]
}

SEED = 42
random.seed(SEED)

def synth_docs(n_per_topic=40):
    docs = []
    for topic, seed in TOPICS.items():
        for i in range(n_per_topic):
            base = random.choice(seed)
            noise = random.choice(seed)
            txt = f"{base} {noise} ({topic} #{i})"
            docs.append({"doc_id": f"{topic}_{i}", "text": txt, "topic": topic})
    return docs

DOCS = synth_docs(40)

def simple_chunk(text, chunk_chars=280, overlap=40):
    out = []
    i = 0
    while i < len(text):
        j = min(len(text), i+chunk_chars)
        out.append((i, j, text[i:j]))
        if j == len(text): break
        i = max(0, j-overlap)
    return out


def build_chunks(docs, chunk_chars=280, overlap=40):
    rows = []
    for d in docs:
        for k,(a,b,txt) in enumerate(simple_chunk(d["text"], chunk_chars, overlap)):
            rows.append({"doc_id": d["doc_id"], "topic": d["topic"], "chunk_id": k, "start": a, "end": b, "chunk": txt})
    return pd.DataFrame(rows)

CHUNK_SIZE=280; OVERLAP=40
df = build_chunks(DOCS, CHUNK_SIZE, OVERLAP)

embedder = SentenceTransformer(MODEL_NAME)
def embed_texts(texts, batch_size=64):
    return embedder.encode(texts, batch_size=batch_size, convert_to_numpy=True, normalize_embeddings=True).astype("float32")

chunks = df["chunk"].tolist()
t0=time.time()
embs = embed_texts(chunks, 64)
build_s = time.time()-t0

index = faiss.IndexFlatIP(embs.shape[1])
index.add(embs)

def tokenize(text: str):
    return re.findall(r"[a-z0-9]+", text.lower())
bm25_corpus = [tokenize(c) for c in chunks]
bm25 = BM25Okapi(bm25_corpus)


def retrieve_dense(query: str, k: int=5):
    q = embed_texts([query], batch_size=1)
    scores, idxs = index.search(q, k)
    return [(float(scores[0][i]), df.iloc[idxs[0][i]].to_dict()) for i in range(k)]


def retrieve_bm25(query: str, k: int=5):
    toks = tokenize(query)
    scores = bm25.get_scores(toks)
    idxs = np.argsort(scores)[::-1][:k]
    return [(float(scores[i]), df.iloc[i].to_dict()) for i in idxs]


print(retrieve_dense("What improves running economy?", 3)[0])
print(retrieve_bm25("What improves running economy?", 3)[0])

GOLDEN = [
    ("How do transformers predict tokens?", "ai"),
    ("What is an embedding used for?", "ai"),
    ("How does RAG work?", "ai"),
    ("How to train for a marathon?", "sport"),
    ("What improves running economy?", "sport"),
    ("What is a threshold workout?", "sport"),
    ("How to feed sourdough starter?", "cooking"),
    ("Why sous vide is precise?", "cooking"),
    ("How to bloom spices?", "cooking"),
    ("How do rivers shape valleys?", "geo"),
    ("What causes earthquakes?", "geo"),
    ("Why do deserts form?", "geo"),
    ("Why is sleep important?", "health"),
    ("Benefits of aerobic exercise?", "health"),
    ("Why eat protein?", "health"),
]


def dcg(rels):
    return sum((rel / math.log2(i + 2) for i, rel in enumerate(rels)))


def ndcg_at_k(rels, k):
    rels_k = rels[:k]
    ideal = sorted(rels_k, reverse=True)
    denom = dcg(ideal) or 1e-9
    return dcg(rels_k) / denom


def eval_query(q, target_topic, retriever, k=5):
    hits = retriever(q, k=k)
    rels = [1 if h[1]["topic"] == target_topic else 0 for h in hits]
    rec = sum(rels) * 1.0
    prec = sum(rels) / len(rels) if rels else 0.0
    rr = 0.0
    for i, r in enumerate(rels, start=1):
        if r==1: rr = 1.0 / i; break
    ndcg = ndcg_at_k(rels, k)
    return {"recall@k": rec, "precision@k": prec, "mrr": rr, "ndcg@k": ndcg}

def evaluate(golden, retriever, k=5):
    rows = []
    for q, t in golden:
        rows.append({"query": q, "topic": t, **eval_query(q, t, retriever, k)})
    return pd.DataFrame(rows)

K = 5
dense_df = evaluate(GOLDEN, retrieve_dense, k=K)
b25_df = evaluate(GOLDEN, retrieve_bm25, k=K)

summary = pd.DataFrame({
    "metric": ["recall@k", "precision@k", "mrr", "ndcg@k"],
    "dense": [dense_df[m].mean() for m in ["recall@k", "precision@k", "mrr", "ndcg@k"]],
    "bm25": [b25_df[m].mean() for m in ["recall@k", "precision@k", "mrr", "ndcg@k"]],
})

print(summary)

def run_settings(chunk_size, overlap, kk):
    diff = build_chunks(DOCS, chunk_size, overlap)
    embs = embedder.encode(
        diff["chunk"].tolist(),
        batch_size=64,
        convert_to_numpy=True,
        normalize_embeddings=True
    ).astype("float32")
    idx = faiss.IndexFlatIP(embs.shape[1]); idx.add(embs)
    def retr(q, k):
        qv = embedder.encode(
            [q],
            convert_to_numpy=True,
            normalize_embeddings=True
        ).astype("float32")
        scores, ids = idx.search(qv, k)
        return [(float(scores[0][i]), diff.iloc[ids[0][i]].to_dict()) for i in range(k)]
    dfres = evaluate(GOLDEN, retr, kk)
    return dfres[["recall@k", "precision@k", "mrr", "ndcg@k"]].mean().to_dict()

"""
grid = []
for cs in [50, 200]:
    for ov in [0, 80]:
        for kk in [3, 5]:
            met = run_settings(cs, ov,kk)
            grid.append({"chunk_size": cs, "overlap": ov, "k": kk, **met})

grid_df = pd.DataFrame(grid).sort_values(["k", "recall@k"], ascending=[True, False])
print(grid_df.head(10))

sub = grid_df[grid_df["k"] == 5].groupby("chunk_size")["recall@k"].mean()
plt.figure()
sub.plot(kind="bar")
plt.title("Recall@5 vs Chunk Size")
plt.xlabel("Chunk Size")
plt.ylabel("Recall@5")
plt.tight_layout()
plt.show()
"""

def log_row(path: str, row: dict):
    exists = os.path.exists(path)
    with open(path, "a", newline='', encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=row.keys())
        if not exists:
            writer.writeheader()
        writer.writerow(row)

log_row("lab10_logs.csv", {
    "timestamp": datetime.now().isoformat(),
    "model_name": MODEL_NAME,
    "index_type": "faiss_flat_ip",
    "k": K,
    "chunk_size": CHUNK_SIZE,
    "overlap": OVERLAP,
    "build_time_sec": build_s,
    "num_chunks": len(df),
    "recall@k_dense": float(summary.loc[summary["metric"]=="recall@k", "dense"].values[0]),
    "recall@k_bm25": float(summary.loc[summary.metric=="recall@k", "bm25"].values[0]),
})

---------------------------------lab7-------------------------------------

# pip install pypdf
import os, time, glob, re, faiss, pandas as pd, numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from pypdf import PdfReader
from google import genai
from dotenv import load_dotenv
from typing import Any, Dict, Optional, List, Tuple

load_dotenv()
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
tokenizer = model = None

SYSTEM_RULES = (
    "You are a factual assistant. Answer ONLY using the provided context snippets. "
    "If missing, reply 'Nie wiem - brak informacji w źródłach.' Include citations like [1], [2]."
)

DOCS = [
    {'id':'d1','text':'Embeddings are vector representations of text used for semantic search.'},
    {'id':'d2','text':'BM25 is a bag-of-words retrieval algorithm based on term frequency.'},
    {'id':'d3','text':'RAG combines retrieval and generation to ground LLM outputs.'},
    {'id':'d4','text':'Cross-encoders score query+doc pairs with a deeper transformer for reranking.'},
    {'id':'d5','text':'Embedding models like all-MiniLM produce compact vectors.'},
]

client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))
print("Using Gemini LLM:", GEMINI_MODEL)

def chat_once_gemini(
    prompt: str,
    system: str = "You are a helpful assistant.",
    temperature: float = 0.3,
    top_p: float = 0.95,
    top_k: int = 40,
    max_output_tokens: int = 1024,
) -> Dict[str, Any]:
    config = genai.types.GenerateContentConfig(
        system_instruction=system,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        max_output_tokens=max_output_tokens,
        stop_sequences=["<END>"],
    )

    t0 = time.time()
    resp = client.models.generate_content(
        model=GEMINI_MODEL,
        contents=prompt,
        config=config,
    )
    dt = round(time.time() - t0, 3)

    usage = getattr(resp, "usage_metadata", None)
    usage_dict: Optional[Dict[str, Any]] = None
    if usage is not None:
        usage_dict = {
            "prompt_tokens": getattr(usage, "prompt_token_count", None),
            "completion_tokens": getattr(usage, "candidates_token_count", None),
            "total_tokens": getattr(usage, "total_token_count", None),
            "tool_use_prompt_tokens": getattr(usage, "tool_use_prompt_token_count", None),
            "thoughts_tokens": getattr(usage, "thoughts_token_count", None),
        }

    return {
        "text": getattr(resp, "text", str(resp)),
        "latency_s": dt,
        "usage": usage_dict
    }

def llm_call(
    prompt: str,
    system: str = "You are a helpful assistant.",
    temperature: float = 0.3,
    top_p: float = 0.95,
    top_k: int = 40,
    max_output_tokens: int = 1024,
) -> Dict[str, Any]:
    return chat_once_gemini(
        prompt,
        system=system,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        max_output_tokens=max_output_tokens
    )

def load_pdf(path):
    rows = []; r=PdfReader(path)
    for i, p in enumerate(r.pages, start=1):
        try:
            text = p.extract_text() or ""
        except Exception:
            text = ""
        rows.append({
            "source": os.path.basename(path),
            "page": i,
            "text": text
        })
    return rows

def load_txt(path):
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        text = f.read()
    return [{
        "source": os.path.basename(path),
        "page": 1,
        "text": text
    }]

def load_md(path):
    return load_txt(path)

def load_corpus(data_dir="data"):
    os.makedirs(data_dir, exist_ok=True)
    rows = []
    for fp in glob.glob(os.path.join(data_dir, "*")):
        l = fp.lower()
        if l.endswith(".pdf"): rows += load_pdf(fp)
        elif l.endswith(".txt"): rows += load_txt(fp)
        elif l.endswith(".md"): rows += load_md(fp)

    if not rows:
        rows = [{
            "source": "demo.md",
            "page": 1,
            "text": "RAG łączy retrieval z generacją."
        }, {
            "source": "demo.md",
            "page": 2,
            "text": "Embeddingi to wektory semantyczne, podobieństwo kosinusowe."
        }]

    return pd.DataFrame(rows)

def simple_chunk(text, chunk_size=500, overlap=50):
    out = []; i = 0
    while i < len(text):
        j = min(len(text), i + chunk_size)
        out.append((i, j, text[i:j]))
        if j == len(text): break
        i = max(0, j - overlap)
    return out

def make_chunks(df, chunk_chars=800, overlap=120):
    rows = []
    for _, r in df.iterrows():
        for k, (a, b, txt) in enumerate(simple_chunk(r["text"], chunk_chars, overlap)):
            if txt.strip():
                rows.append({
                    "source": r["source"],
                    "page": r["page"],
                    "chunk_id": k + 1,
                    "chunk": txt.strip(),
                    "start": a,
                    "end": b
                })
    return pd.DataFrame(rows)

docs_df = load_corpus('./data')
chunks_df = make_chunks(docs_df, 800, 120)
print(f"Loaded {len(docs_df)} documents, {len(chunks_df)} chunks")

EMB_MODEL = os.getenv("EMB_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
embedder = SentenceTransformer(EMB_MODEL)

def embed_texts(texts, batch_size=32):
    embeddings = embedder.encode(
        texts,
        batch_size=batch_size,
        convert_to_numpy=True,
        normalize_embeddings=True
    ).astype("float32")
    return embeddings
embs = embed_texts(chunks_df["chunk"].tolist())
index = faiss.IndexFlatIP(embs.shape[1]); index.add(embs)
print(f"FAISS index: {index.ntotal} vectors of dimension {embs.shape[1]}")

def tok(x): return re.findall(r"[a-ząćęłńóśźż0-9]+", x.lower())
bm25_corpus = [tok(c) for c in chunks_df["chunk"].tolist()]
bm25 = BM25Okapi(bm25_corpus)
print("BM25 index created.")


def retrieve_dense(query, k=5):
    qv = embed_texts([query], batch_size=1)
    scores, idxs = index.search(qv, k)
    return [(float(scores[0][i]), chunks_df.iloc[idxs[0][i]].to_dict()) for i in range(k)]

def pack_context(hits, max_per_source=2, max_chars=2000):
    per = {}; ordered = []
    for _, rec in hits:
        key = (rec["source"], rec["page"])
        per.setdefault(key, 0)
        if per[key] < max_per_source:
            per[key] += 1
            ordered.append(rec)
    cites = []; parts = []
    for i, rec in  enumerate(ordered, start=1):
        cites.append({
            "n": i,
            "source": rec["source"],
            "page": rec["page"],
            "chunk_id": rec["chunk_id"]
        })
        parts.append(f"[{i}] {rec['chunk']}")
    ctx = "\n\n".join(parts)
    return (ctx[:max_chars], cites)

def answer_with_api(question, hits):
    ctx, cites = pack_context(hits)
    prompt = "Question: " + question + "\n\nContext:\n" + ctx + "\n\nAnswer in Polish with citations [n]."
    return llm_call(prompt, system=SYSTEM_RULES, max_output_tokens=512, temperature=0.0), cites

print(answer_with_api("Jakie ery wyróżniamy?", retrieve_dense('ery', k=5)))
print(answer_with_api("Co to jest RAG?", retrieve_dense('RAG', k=5)))

# -------------- Od tego miejsca reraking z cross-encoderem --------------

texts = [d['text'] for d in DOCS]
embs = embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True).astype("float32")
idx = faiss.IndexFlatIP(embs.shape[1]); idx.add(embs)
bm25_corpus = [tok(t) for t in texts]
bm25 = BM25Okapi(bm25_corpus)

def retrieve_rerank(query, k=5):
    qv = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype("float32")
    scores, idxs = idx.search(qv, k)
    return [(float(scores[0][i]), DOCS[idxs[0][i]]) for i in range(min(k, len(idxs[0])))]

USE_CROSS_ENCODER = False
try:
    from sentence_transformers import CrossEncoder
    CROSS_ENCODER = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')
    USE_CROSS_ENCODER = True
except Exception:
    CROSS_ENCODER = None
    USE_CROSS_ENCODER = False

def rerank(query: str, candidates: List[Tuple[float, Dict[str, Any]]]) -> List[Tuple[float, Dict[str, Any]]]:
    if USE_CROSS_ENCODER and CROSS_ENCODER is not None:
        pairs = [[query, c[1]['text']] for c in candidates]
        scores = CROSS_ENCODER.predict(pairs)
        scored = [(float(s), c[1]) for s, c in zip(scores, candidates)]
        scored.sort(key=lambda x: x[0], reverse=True)
        return scored

    qv = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype("float32")[0]
    out = []
    for score, doc in candidates:
        idx_doc = next((i for i, d in enumerate(DOCS) if d['id'] == doc['id']), None)
        if idx_doc is not None:
            s = score
        else:
            s = float(np.dot(qv, embs[idx_doc]))
        out.append((s, doc))
    out.sort(key=lambda x: x[0], reverse=True)
    return out

query = "What are embeddings used for?"
print('BM25 top-3 results:')
bm = bm25.get_scores(tok(query));
bm_idx = np.argsort(bm)[::-1][:3]
for i in bm_idx:
    print(f"Score: {bm[i]:.4f}, Texts: {texts[i]}")

print('\nDense retrieval top-3 results:')
cand = retrieve_rerank(query, k=3)
for s, d in cand:
    print(f"Score: {s:.4f}, Texts: {d['text']}")

print('\nReranked results:')
rr = rerank(query, cand)
for s, d in rr:
    print(f"Score: {s:.4f}, Texts: {d['text']}")

GOLD = [
    ('what are embeddings used for?', {'relevant_ids':['d1','d5']}),
    ('how does RAG work?', {'relevant_ids':['d3']}),
]

def recall_at_k(ranked_docs, relevant_ids, k=3):
    topk = [d['id'] for _, d in ranked_docs[:k]]
    return len(set(topk) & set(relevant_ids)) / len(relevant_ids)

def evaluate_rerank():
    rows = []
    for q, meta in GOLD:
        baseline = retrieve_rerank(q, k=5)
        baseline_recall = recall_at_k(baseline, meta['relevant_ids'], k=3)
        reranked = rerank(q, baseline)
        rerank_recall = recall_at_k(reranked, meta['relevant_ids'], k=3)
        rows.append({
            "query": q,
            "baseline_recall@3": baseline_recall,
            "rerank_recall@3": rerank_recall
        })
    return pd.DataFrame(rows)

print(evaluate_rerank())

----------------------------------lab0-----------------------------------------

import os, time
from platform import system
from typing import List

import torch, google.generativeai as genai
import tiktoken
from dotenv import load_dotenv
from openai import OpenAI
from transformers import AutoTokenizer, AutoModelForCausalLM
load_dotenv()


GROQ_API_KEY = os.getenv("GROQ_API_KEY", None)
BASE_URL = "https://api.groq.com/openai/v1"
GROQ_MODEL_NAME = os.getenv("GROQ_MODEL_NAME", "openai/gpt-oss-20b")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flesh")
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
LOCAL_MODEL_NAME = os.getenv("LOCAL_MODEL_NAME", "Qwen/Qwen2.5-0.5B-Instruct")

tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_NAME,
    dtype=torch.float16 if torch.cuda.is_available() else torch.float32
)
client = OpenAI(
    api_key= GROQ_API_KEY,
    base_url=BASE_URL,
)
def chat_once_openai(prompt: str, system: str, temperatura: float=0.3, top_p: float=0.95, top_k: int=40, max_output_tokens: int=512):
    t0 = time.time()
    response = client.chat.completions.create(
        model=GROQ_MODEL_NAME,
        messages=[{"role": "user", "content": prompt},
                  {"role": "system", "content": system}
                  ],
        temperature=temperatura,
        top_p=top_p,
        max_tokens=max_output_tokens
    )
    dt = time.time() - t0
    choice = response.choices[0]
    out = {
        "text": choice.message.content,
        'finish_reason': choice.finish_reason,
        'latency_s' : round(dt, 3),
        'usage': getattr(response, 'usage', None) and response.usage.model_dump()
    }
    return out

def approx_tokens_openai(texts: List[str], model_name: str="gpt-40-mini") -> int:
    try:
        enc = tiktoken.encoding_for_model(model_name)
        return sum(len(enc.encode(t)) for t in texts)
    except Exception as e:
        return sum(max(1, len(t) // 4) for t in texts)
def estimate_cost_use(prompt_tokens: int, completion_tokens: int, price_in: float = 0.000005, price_out: float = 0.0000015) -> float:

    return prompt_tokens * price_in + completion_tokens * price_out



input_txt = "jaki sposób rozbudować mieśnie brzucha najbardziej efektywnie"

result = chat_once_openai(
    "w jaki sposób rozbudować mieśnie brzucha najbardziej efektywnie",
    "jesteś pomocnym trenerem odpowiadaj, zwięźle po polsku",
    1,
    0.95,
    40,
    512
)

output_txt = result['text']
ptoks = approx_tokens_openai('input_txt')
otoks = approx_tokens_openai('output_txt')

print(ptoks, otoks)
print(estimate_cost_use(ptoks, otoks))
print(result['usage'])
"""
print((result['text']))
print("finish_reason", (result['finish_reason']))
print("latency_s", (result['latency_s']))
print("usage", (result['usage']))
"""





"""
def chat_once_gemini(prompt: str, system: str="You are helpful assistant.",
                    temperatura: float=0.3, top_p: float=0.95, top_k: int=40, max_output_tokens: int=512) -> str:
    config = genai.GenerationConfig(
        temperature=temperatura,
        top_k=top_k,
        top_p=top_p,
        max_output_tokens=max_output_tokens,
        stop_sequences=["<END>"]
    )
    t0 = time.time()

    model = genai.GenerativeModel(
        GEMINI_MODEL,
        system_instruction=system,
        generation_config=config

    )
    resp = model.generate_content(prompt)
    dt = time.time() - t0
    return {
        "text": getattr(resp, "text", str(resp)),
        "latency_s": dt,
        "usage": {
            "completion_token": resp.usage_metadata.candidates_token_count,
            "prompt_token_count": resp.usage_metadata.prompt_token_count,
            "total_token": resp.usage_metadata.total_token_count
        }
    }

result = chat_once_gemini(
    "w jaki sposób rozbudować mieśnie brzucha najbardziej efektywnie",
    "jesteś pomocnym trenerem odpowiadaj, zwięźle po polsku",
    1,
    0.95,
    40,
    512
)

print((result['text']))


"""